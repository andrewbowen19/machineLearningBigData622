RMSE(predictTree, testData$y)
varImp(marsTuned)
# Train spline model (MARS)
marsGrid <- expand.grid(.degree = 1:2, .nprune = 2:38)
marsTuned <- train(x=trainingData$x,
y=trainingData$y, method = "earth", tuneGrid = marsGrid,
trControl = trainControl(method = "cv"))
marsTuned
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(caret)
library(tidyverse)
library(caret)
library(mlbench)
set.seed(200)
simulated <- mlbench.friedman1(200, sd = 1) > simulated <- cbind(simulated$x, simulated$y) > simulated <- as.data.frame(simulated)
library(mlbench)
set.seed(200)
simulated <- mlbench.friedman1(200, sd = 1) > simulated <- cbind(simulated$x, simulated$y)
library(mlbench)
set.seed(200)
simulated <- mlbench.friedman1(200, sd = 1)
simulated <- cbind(simulated$x, simulated$y)
simulated <- as.data.frame(simulated)
colnames(simulated)[ncol(simulated)] <- "y"
library(tidyverse)
library(caret)
library(randomForest)
model1 <- randomForest(y ~ ., data = simulated, importance = TRUE,
ntree = 1000) rfImp1 <- varImp(model1, scale = FALSE)
model1 <- randomForest(y ~ ., data = simulated, importance = TRUE,
ntree = 1000)
rfImp1 <- varImp(model1, scale = FALSE)
model1 <- randomForest(y ~ ., data = simulated, importance = TRUE,
ntree = 1000)
rfImp1 <- varImp(model1, scale = FALSE)
rfImp1
simulated$duplicate1 <- simulated$V1 + rnorm(200) * .1
cor(simulated$duplicate1, simulated$V1)
# Add correlated predictor
simulated$duplicate1 <- simulated$V1 + rnorm(200) * .1
# Fit separate random forest variable
model2 <- randomForest(y ~ ., data = simulated, importance = TRUE, ntree = 1000)
rfImp2 <- varImp(model2, scale = FALSE)
rfImp2
# Add correlated predictor
simulated$duplicate2 <- simulated$V2 + rnorm(200) * .1
# Fit separate random forest variable
model3 <- randomForest(y ~ ., data = simulated, importance = TRUE, ntree = 1000)
rfImp3 <- varImp(model3, scale = FALSE)
rfImp3
# Add correlated predictor
simulated$duplicate2 <- simulated$V1 + rnorm(200) * .1
# Fit separate random forest variable
model3 <- randomForest(y ~ ., data = simulated, importance = TRUE, ntree = 1000)
rfImp3 <- varImp(model3, scale = FALSE)
rfImp3
library(cforest)
install.packages("cforest")
install.packages("party")
library(cforest)
library(cforest)
library(cforest)
install.packages("cforest")
# library(cforest)
library(party)
bagCtrl <- cforest_control(mtry = ncol(simulated) - 1)
baggedTree <- cforest(y ~ ., data = simulated, controls = bagCtrl)
# Traing conditional inference tree
bagCtrl <- cforest_control(mtry = ncol(simulated) - 1)
baggedTree <- cforest(y ~ ., data = simulated, controls = bagCtrl)
# Traing conditional inference tree
bagCtrl <- cforest_control(mtry = ncol(simulated) - 1)
baggedTree <- cforest(y ~ ., data = simulated, controls = bagCtrl)
varimp(baggedTree)
# Traing conditional inference tree
bagCtrl <- cforest_control(mtry = ncol(simulated) - 1)
baggedTree <- cforest(y ~ ., data = simulated, controls = bagCtrl)
varimp(baggedTree, conditional-TRUE)
# Traing conditional inference tree
bagCtrl <- cforest_control(mtry = ncol(simulated) - 1)
baggedTree <- cforest(y ~ ., data = simulated, controls = bagCtrl)
varimp(baggedTree)
?varimp
# Traing conditional inference tree
bagCtrl <- cforest_control(mtry = ncol(simulated) - 1)
baggedTree <- cforest(y ~ ., data = simulated, controls = bagCtrl)
varimp(baggedTree, conditional=TRUE)
install.packages("Cubist")
# Cubist
# library(Cubist)
cubistTuned <- train(simulated[, -c("y")], simulated$y, method = "cubist")
# Cubist
# library(Cubist)
cubistTuned <- train(simulated %>% select(-c("y")), simulated$y, method = "cubist")
# Cubist
predictors <- simulated %>% select(-c("y"))
cubistTuned <- train(predictors, simulated$y, method = "cubist")
# Boosted
gbmGrid <- expand.grid(.interaction.depth = seq(1, 7, by = 2), .n.trees = seq(100, 1000, by = 50),
gbmTune <- train(predictors, simulated$y,method = "gbm",
tuneGrid = gbmGrid,verbose = FALSE)
# Cubist
predictors <- simulated %>% select(-c("y"))
cubistTuned <- train(predictors, simulated$y, method = "cubist")
# Boosted
gbmGrid <- expand.grid(.interaction.depth = seq(1, 7, by = 2), .n.trees = seq(100, 1000, by = 50), .shrinkage=c(0.1, 1.0))
gbmTune <- train(predictors, simulated$y, method = "gbm",
tuneGrid = gbmGrid,verbose = FALSE)
library(tidyverse)
library(caret)
library(party)
library(randomForest)
library(gbm)
install.packages("gbm")
library(tidyverse)
library(caret)
library(party)
library(randomForest)
library(gbm)
install.packages("gbm")
library(tidyverse)
library(caret)
library(party)
library(randomForest)
library(gbm)
install_github("SimonDedman/gbm.auto", force = TRUE)
library(tidyverse)
library(caret)
library(party)
library(randomForest)
library(gbm)
# Cubist model
predictors <- simulated %>% select(-c("y"))
cubistTuned <- train(predictors, simulated$y, method = "cubist")
# Boosted model
gbmGrid <- expand.grid(.interaction.depth = seq(1, 7, by = 2), .n.trees = seq(100, 1000, by =100), .shrinkage=c(0.1, 1.0))
gbmTune <- train(predictors, simulated$y, method = "gbm",
tuneGrid = gbmGrid,verbose = FALSE)
# Cubist model
predictors <- simulated %>% select(-c("y"))
cubistTuned <- train(predictors, simulated$y, method = "cubist")
# Boosted model
gbmGrid <- expand.grid(.interaction.depth = seq(1, 7, by = 2), .n.trees = seq(100, 1000, by =100), .shrinkage=c(0.1, 1.0))
gbmTune <- train(predictors, simulated$y, method = "gbm",
tuneGrid = gbmGrid,verbose = FALSE)
library(Cubist)
# Cubist model
predictors <- simulated %>% select(-c("y"))
cubistTuned <- cubist(predictors, simulated$y)
# Boosted model
gbmGrid <- expand.grid(.interaction.depth = seq(1, 7, by = 2), .n.trees = seq(100, 1000, by =100), .shrinkage=c(0.1, 1.0))
gbmTune <- train(predictors, simulated$y, method = "gbm",
tuneGrid = gbmGrid,verbose = FALSE)
# Boosted model
gbmGrid <- expand.grid(.interaction.depth = seq(1, 7, by = 2), .n.trees = seq(100, 1000, by =100), .shrinkage=c(0.1, 1.0))
gbmTune <- train(predictors, simulated$y, method = "gbm",
tuneGrid = gbmGrid,verbose = FALSE)
# Boosted model
gbmGrid <- expand.grid(.interaction.depth = seq(1, 7, by = 2), n.trees = seq(100, 1000, by =100), shrinkage=c(0.1, 1.0))
gbmTune <- train(predictors, simulated$y, method = "gbm",
tuneGrid = gbmGrid,verbose = FALSE)
# Boosted model
gbmGrid <- expand.grid(.interaction.depth = seq(1, 7, by = 2),
n.trees = seq(100, 1000, by =100),
shrinkage=c(0.1, 1.0))
gbmTune <- train(predictors, simulated$y, method = "gbm",
tuneGrid = gbmGrid,verbose = FALSE)
# Boosted model
gbmGrid <- expand.grid(interaction.depth = seq(1, 7, by = 2),
n.trees = seq(100, 1000, by =100),
shrinkage=c(0.1, 1.0))
gbmTune <- train(predictors, simulated$y, method = "gbm",
tuneGrid = gbmGrid,verbose = FALSE)
# Boosted model
gbmGrid <- expand.grid(interaction.depth = seq(1, 7, by = 2),
n.trees = seq(100, 1000, by =100),
shrinkage=c(0.1, 1.0),
n.minobsinnode=10)
gbmTune <- train(predictors, simulated$y, method = "gbm",
tuneGrid = gbmGrid,verbose = FALSE)
library(Cubist)
# Cubist model
predictors <- simulated %>% select(-c("y"))
cubistTuned <- cubist(predictors, simulated$y)
varImp(cubistTuned)
# Boosted model
gbmGrid <- expand.grid(interaction.depth = seq(1, 5, by = 2),
n.trees = seq(100, 1000, by = 200),
shrinkage=c(0.1, 1.0),
n.minobsinnode=seq(1, 5, 2))
gbmTune <- train(predictors, simulated$y, method = "gbm",
tuneGrid = gbmGrid,verbose = FALSE)
varImp(gbmTuned)
# Boosted model
gbmGrid <- expand.grid(interaction.depth = seq(1, 5, by = 2),
n.trees = seq(100, 1000, by = 200),
shrinkage=c(0.1, 1.0),
n.minobsinnode=seq(1, 5, 2))
gbmTune <- train(predictors, simulated$y, method = "gbm",
tuneGrid = gbmGrid,verbose = FALSE)
varImp(gbmTuned)
# Boosted model
gbmGrid <- expand.grid(interaction.depth = seq(1, 5, by = 2),
n.trees = seq(100, 1000, by = 200),
shrinkage=c(0.1, 1.0),
n.minobsinnode=seq(1, 5, 2))
gbmTuned <- train(predictors, simulated$y, method = "gbm",
tuneGrid = gbmGrid,verbose = FALSE)
varImp(gbmTuned)
# Re-sumlate data from Friedman
```
# Re-sumlate data from Friedman
set.seed(12345)
simulated <- mlbench.friedman1(200, sd = 1)
# Re-sumlate data from Friedman
set.seed(12345)
simulated <- mlbench.friedman1(150, sd = 1)
# Re-sumlate data from Friedman
set.seed(12345)
simulated2 <- mlbench.friedman1(150, sd = 1)
library(rpart)
# Re-sumlate data from Friedman
set.seed(12345)
simulated2 <- mlbench.friedman1(150, sd = 1)
rpartTree <- rpart(y ~ ., data = simulated2)
View(simulated2)
library(rpart)
# Re-sumlate data from Friedman
set.seed(12345)
simulated2 <- mlbench.friedman1(150, sd = 1)
simulated2 <- cbind(simulated2$x, simulated2$y)
simulated2 <- as.data.frame(simulated2)
colnames(simulated2)[ncol(simulated2)] <- "y"
library(rpart)
# Re-sumlate data from Friedman
set.seed(12345)
simulated2 <- mlbench.friedman1(150, sd = 1)
simulated2 <- cbind(simulated2$x, simulated2$y)
simulated2 <- as.data.frame(simulated2)
colnames(simulated2)[ncol(simulated2)] <- "y"
rpartTree <- rpart(y ~ ., data = simulated2)
trainX <- simulated2 %>% select(-c("y"))
trainY <- simulated$y
rpartTune <- train(trainX, trainY, method = "rpart2",
tuneLength = 10,
trControl = trainControl(method = "cv"))
trainX <- simulated2 %>% select(-c("y"))
trainY <- simulated$y
rpartTune <- train(trainX, trainY, method = "rpart2",
tuneLength = 10,
trControl = trainControl(method = "cv"))
varimp(rpartTune)
trainX <- simulated2 %>% select(-c("y"))
trainY <- simulated$y
rpartTune <- train(trainX, trainY, method = "rpart2",
tuneLength = 10,
trControl = trainControl(method = "cv"))
varImp(rpartTune)
# Load chemical data
library(AppliedPredictiveModeling)
library(kernlab)
library(earth)
library(nnet)
library(ModelMetrics)
data("ChemicalManufacturingProcess")
chemical <- ChemicalManufacturingProcess
chemical_features <- chemical %>% dplyr::select(-c("Yield"))
# Impute chemical yield data
imputed <- preProcess(chemical,
method = c("knnImpute"))
trans <- predict(imputed, chemical)
train(train, train_yield, method = "gbm",
tuneGrid = gbmGrid,verbose = FALSE)
# Split into train and test splits
#use 75% of dataset as training set and 30% as test set
sample <- sample(c(TRUE, FALSE), nrow(trans), replace=TRUE, prob=c(0.8,0.2))
train  <- trans[sample, ]
train_yield <- train$Yield
train <- train %>%
dplyr::select(-c("Yield"))
test <- trans[!sample, ]
test_yield <- test$Yield
test <- test  %>%
dplyr::select(-c("Yield"))
train(train, train_yield, method = "gbm",
tuneGrid = gbmGrid, verbose = FALSE)
gbmReg <- train(train, train_yield, method = "gbm",
tuneGrid = gbmGrid, verbose = FALSE)
library(modelr)
rmse(gbmReg)
rmse(gbmReg, test)
?rmse
rmse(predict(test, test_yield))
gbmPred <- predict(gbmReg, test)
rmse(gbmPredt, test_yield)
gbmPred <- predict(gbmReg, test)
rmse(gbmPred, test_yield)
gbmPred <- predict(gbmReg, test)
rmse(test_yield, gbmPred)
gbmPred <- predict(gbmReg, test)
ModelMetrics::rmse(test_yield, gbmPred)
rpartReg <- train(train, train_yield, method = "rpart",
tuneGrid = gbmGrid, verbose = FALSE)
rpartReg <- train(train, train_yield, method = "rpart",
verbose = FALSE)
rpartReg <- train(train, train_yield, method = "rpart2",
tuneLength = 10,
trControl = trainControl(method = "cv"), verbose = FALSE)
rpartReg <- train(train, train_yield, method = "rpart2",
tuneLength = 10,
trControl = trainControl(method = "cv"))
# Calculate RMSE of gradient boosted tree
rpartPred <- predict(rpartReg, test)
ModelMetrics::rmse(test_yield, rpartPred)
library(randomForest)
rfModel <- randomForest(solTrainXtrans, solTrainY)
library(randomForest)
rfModel <- randomForest(train, train_yield)
library(randomForest)
rfModel <- randomForest(train, train_yield)
predict(rfModel, test)
library(randomForest)
rfModel <- randomForest(train, train_yield)
rfPred <- predict(rfModel, test)
ModelMetrics::rmse(test_yield, rfPred)
gbmReg <- train(train, train_yield, method = "gbm",
tuneGrid = gbmGrid, verbose = FALSE)
# Calculate RMSE of gradient boosted tree
gbmPred <- predict(gbmReg, test)
ModelMetrics::rmse(test_yield, gbmPred)
library(randomForest)
rfModel <- randomForest(train, train_yield)
rfPred <- predict(rfModel, test)
ModelMetrics::rmse(test_yield, rfPred)
varImp(rfModel)
knitr::opts_chunk$set(echo = TRUE)
# Load chemical data
library(AppliedPredictiveModeling)
library(kernlab)
library(earth)
library(nnet)
library(ModelMetrics)
data("ChemicalManufacturingProcess")
chemical <- ChemicalManufacturingProcess
chemical_features <- chemical %>% dplyr::select(-c("Yield"))
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(MASS)
library(caret)
library(mlbench)
library(xgboost)
library(GGally)
library(e1071)
library(corrplot)
set.seed(200)
trainingData <- mlbench.friedman1(200, sd = 1)
trainingData$x <- data.frame(trainingData$x)
featurePlot(trainingData$x, trainingData$y)
# Set up test data
testData <- mlbench.friedman1(5000, sd = 1)
testData$x <- data.frame(testData$x)
# Plot training data distributions
ggpairs(cbind(trainingData$x, trainingData$y))
# library(caret)
knnModel <- train(x = trainingData$x,
y = trainingData$y,
method = "knn",
preProc = c("center", "scale"))
knnModel
knnPred <- predict(knnModel, testData$x)
RMSE(knnPred, testData$y)
decisionTreeModel <- train(x=trainingData$x,
y=trainingData$y,
method="rpart",
tuneLength=10)
decisionTreeModel
# Predict using decision tree
predictTree <- predict(decisionTreeModel, testData$x)
RMSE(predictTree, testData$y)
# Train spline model (MARS)
marsGrid <- expand.grid(.degree = 1:2, .nprune = 2:38)
marsTuned <- train(x=trainingData$x,
y=trainingData$y, method = "earth", tuneGrid = marsGrid,
trControl = trainControl(method = "cv"))
marsTuned
predictMars <- predict(marsTuned, testData$x)
RMSE(predictMars, testData$y)
varImp(marsTuned)
# Train XGBoost model
xgBoosetModel <- xgboost(data = as.matrix(trainingData$x),
label = as.matrix(trainingData$y),
max.depth = 2, eta = 1, nthread = 2,
nrounds = 2)
xgBoosetModel
# Get top-10 feature names
importance <- importance$importance
plot(rpartReg)
library(rpart.plot)
library(rpart.plot)
rpart.plot(rpartReg)
library(rpart.plot)
rpartTree <- rpart(Yield ~ ., data = train, method = "reg")
train
colnames(train)
library(rpart.plot)
rpartTree <- rpart(Yield ~ ., data = cbind(train, train_yield), method = "reg")
library(rpart.plot)
rpartTree <- rpart(yield ~ ., data = cbind(train, train_yield), method = "reg")
cbind(train, train_yield)
library(rpart.plot)
rpartTree <- rpart(train_yield ~ ., data = cbind(train, train_yield), method = "reg")
?rpart
library(rpart.plot)
rpartTree <- rpart(train_yield ~ ., data = cbind(train, train_yield))#, method = "reg")
rpart.plot(rpartTree)
varImp(rpartTune)
varImp(rpartTune)
trainX <- simulated2 %>% select(-c("y"))
trainX <- simulated2 %>% select(-c("y"))
library(rpart)
# Re-sumlate data from Friedman
set.seed(12345)
simulated2 <- mlbench.friedman1(150, sd = 1)
simulated2 <- cbind(simulated2$x, simulated2$y)
simulated2 <- as.data.frame(simulated2)
colnames(simulated2)[ncol(simulated2)] <- "y"
trainX <- simulated2 %>% select(-c("y"))
View(simulated2)
simulated2 %>% select(-c("y"))
simulated2 %>% select(-c(y))
simulated2 %>% dply::select(-c("y"))
trainX <- simulated2 %>% dplyr::select(-c("y"))
trainY <- simulated$y
trainX <- simulated2 %>% dplyr::select(-c("y"))
trainY <- simulated2$y
rpartTune <- train(trainX, trainY, method = "rpart2",
tuneLength = 10,
trControl = trainControl(method = "cv"))
varImp(rpartTune)
?plotvarImp
varImp(rfModel)
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(caret)
library(mice)
library(ROCR)
elo <- read.csv("data/nfl_elo.csv")
# Some basic handling of team renames, as well as a boolean winner column
elo <- elo %>%
mutate(team1 = ifelse(team1=="WSH", "WAS", team1),
team2 = ifelse(team2=="WSH", "WAS", team2),
winner = ifelse(score1 > score2, "Home", "Away"))
elo$winner <- as.factor(elo$winner)
nba <- read_csv("data/traditional.csv")
head(nba)
# Impute all values in our training data
input_cols <- c("elo1_pre", "elo2_pre",
"elo_prob1", "elo_prob2",
"qbelo1_pre", "qbelo2_pre",
"qb1_value_pre", "qb2_value_pre",
"qb1_adj", "qb2_adj")
naive_inputs <- elo[, input_cols ]
imputed_qb_ratings <- mice(naive_inputs, meth='pmm')
imputedData <- complete(imputed_qb_ratings, 1)
imputedData$winner <- elo$winner
# Create train-test split
set.seed(1234)
# create ID column
imputedData$game_id <- 1:nrow(imputedData)
# use 70% of dataset as traininging set and 30% as test set
train <- imputedData %>% dplyr::sample_frac(0.70)
test  <- dplyr::anti_join(imputedData, training, by = 'game_id')
predictions <- predict(svmLinear, test, decision.values = TRUE)
trainControl <- trainControl(method="repeatedcv", number=10, repeats=3)
svmLinear <- train(winner ~., data = train,
method = "svmLinear",
trControl = trainControl,
preProcess = c("center","scale"))
predictions <- predict(svmLinear, test, decision.values = TRUE)
# Create train-test split
set.seed(1234)
# create ID column
imputedData$game_id <- 1:nrow(imputedData)
# use 70% of dataset as traininging set and 30% as test set
train <- imputedData %>% dplyr::sample_frac(0.70)
test  <- dplyr::anti_join(imputedData, training, by = 'game_id')
# Create train-test split
set.seed(1234)
# create ID column
imputedData$game_id <- 1:nrow(imputedData)
# use 70% of dataset as traininging set and 30% as test set
train <- imputedData %>% dplyr::sample_frac(0.70)
test  <- dplyr::anti_join(imputedData, train, by = 'game_id')
predictions <- predict(svmLinear, test, decision.values = TRUE)
roc_score=roc(test$winner, predictions) #AUC score
library(ROCR)
library(pROC)
predictions <- predict(svmLinear, test, decision.values = TRUE)
roc_score=roc(test$winner, predictions) #AUC score
predictions <- predict(svmLinear, test, decision.values = TRUE)
summary(svmLinear)
predictions <- predict(svmLinear, test, decision.values = TRUE)
confusionMatrix(test$winner, predictions)
svmPoly <- train(winner ~., data = train,
method = "svmPoly",
trControl = trainControl,
preProcess = c("center","scale"))
svmPoly <- train(winner ~., data = train,
method = "svmPoly",
trControl = trainControl,
preProcess = c("center","scale"))
